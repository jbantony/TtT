{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "cite2c": {
      "citations": {
        "7429414/A3M3BHVX": {
          "URL": "http://archive.ics.uci.edu/ml/datasets/Gas+Sensor+Array+Drift+Dataset+at+Different+Concentrations",
          "accessed": {
            "day": 14,
            "month": 12,
            "year": 2019
          },
          "id": "7429414/A3M3BHVX",
          "title": "UCI Machine Learning Repository: Gas Sensor Array Drift Dataset at Different Concentrations Data Set",
          "type": "webpage"
        },
        "7429414/F5MSIETD": {
          "URL": "https://scikit-learn.org/stable/modules/feature_selection.html",
          "accessed": {
            "day": 14,
            "month": 12,
            "year": 2019
          },
          "id": "7429414/F5MSIETD",
          "title": "1.13. Feature selection — scikit-learn 0.22 documentation",
          "type": "webpage"
        },
        "7429414/P3YHU5HB": {
          "URL": "https://github.com/pandas-profiling/pandas-profiling",
          "accessed": {
            "day": 14,
            "month": 12,
            "year": 2019
          },
          "id": "7429414/P3YHU5HB",
          "title": "pandas-profiling/pandas-profiling: Create HTML profiling reports from pandas DataFrame objects",
          "type": "webpage"
        },
        "7429414/PJ6TJEXS": {
          "URL": "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html",
          "accessed": {
            "day": 16,
            "month": 12,
            "year": 2019
          },
          "id": "7429414/PJ6TJEXS",
          "title": "Indexing and selecting data — pandas 0.25.3 documentation",
          "type": "webpage"
        },
        "7429414/WP7NDQ54": {
          "URL": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py",
          "accessed": {
            "day": 14,
            "month": 12,
            "year": 2019
          },
          "id": "7429414/WP7NDQ54",
          "title": "Visualizing cross-validation behavior in scikit-learn — scikit-learn 0.22 documentation",
          "type": "webpage"
        },
        "7429414/YMEC4PBD": {
          "URL": "https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/",
          "abstract": "Pandas Data Selection There are multiple ways to select and index rows and columns from Pandas DataFrames. I find tutorials online focusing on advanced selections of row and column choices a little…",
          "accessed": {
            "day": 14,
            "month": 12,
            "year": 2019
          },
          "author": [
            {
              "family": "shanelynn",
              "given": ""
            }
          ],
          "container-title": "Shane Lynn",
          "id": "7429414/YMEC4PBD",
          "issued": {
            "day": 2,
            "month": 10,
            "year": 2016
          },
          "language": "en-US",
          "title": "Using iloc, loc, & ix to select rows and columns in Pandas DataFrames",
          "type": "webpage"
        }
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "03_Daten_clean_Data.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyfhIfcuzrxB"
      },
      "source": [
        "# Modul Daten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9127QPMlzrxC"
      },
      "source": [
        "Dieses Jupyter Notebook soll einen Überblick über die Aufbereitung, Bereinigung und Visualisierung von Daten vermitteln. \n",
        "\n",
        "Für diesen Zweck wurde die Datenbank Gas Sensor Array Drift verwendet, heruntergeladen und vorher für diesen Zweck manipuliert (Gas- und Konzentrationsklassenlabel wurden zu einem Label zusammen geführt und dabei drei verschiedene Konzentrationsstufen definiert: low, medium und high). <br>\n",
        "Der vollständige Datensatz kann auf <br>\n",
        "\n",
        "http://archive.ics.uci.edu/ml/datasets/Gas+Sensor+Array+Drift+Dataset+at+Different+Concentrations\n",
        "\n",
        "heruntergeladen werden. Außerdem werden verschiedene Informationen zu den Daten bereitgestellt. <br>\n",
        "\n",
        "Der Datensatz enthält 13.910 Messungen von 16 chemischen Sensoren, welche abwechselnd sechs verschiedene Gase (Ethanol, Ethen, Ammoniak, Acetaldehyd, Aceton und Toluol) in verschiedenen Konzentrationen untersuchen. <br>\n",
        "Der Datensatz wurde zwischen Januar 2008 und Februar 2011 (36 Monaten) erhoben. <br>\n",
        "Jede Messung liefert anhand der 16 vorhandenen Sensoren eine 16-Kanal-Zeitreihe. <br>\n",
        "Es werden zwei Hauptfeatures in diesem Datensatz betrachtet: <br>\n",
        "$\\rightarrow~$ (i) das stationäre Feature (steady-state) bezeichnet als `DR`, definiert als die maximale Widerstandsänderung in Bezug auf eine Basis, sowie die normalisierte Version davon (`|DR|`). <br>\n",
        "$\\rightarrow~$ (ii) ein Ansammlung an Features welche die Sensordynamik der gesamten Messung wiederspiegelt (`EMAi` und `EMAd` für verschiedene $\\alpha$-Werte). <br>\n",
        "\n",
        "Im folgendem Beispiel werden wir uns nur mit dem `DR` Feature auseinandersetzen. Da jeder der 16 Sensoren diesen Messwert liefert, sind auch 16 verschiedene Messwerte von `DR` vorhanden \"(`DR_1` des ersten Sensors, `DR_2`, des zweiten Sensors und so weiter). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akjf4TQgzrxE"
      },
      "source": [
        "## Daten laden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s629Z-lAzrxG"
      },
      "source": [
        "Als erstes müssen alle Beispieldatensätze geladen werden. Dieser liegen in 10 verschiedene csv Dateien vor (batch0.csv bis batch9.csv). Anschließend werden diese zu einem gemeinsamen Datensatz zusammengeführt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci_Pv-VzzrxH"
      },
      "source": [
        "!git clone https://github.com/r1marcus/TraintheTrainer-Daten.git\n",
        "!echo $CWD\n",
        "!cp -rf /content/TraintheTrainer-Daten/* /content/\n",
        "\n",
        "import scripts.load\n",
        "df_modified, df_original,all_files_mod,all_files_original = scripts.load.load()\n",
        "import scripts.splitt\n",
        "df_training, df_test = scripts.splitt.splitt(all_files_mod,70,30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz-oZuhUzrxR"
      },
      "source": [
        "## Daten Bereinigen und Transformieren"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0yIn0ZizrxS"
      },
      "source": [
        "In diesem Abschnitt soll eine sehr elegante Methode zur Bereinigung und Transformation der Daten vorgestellt werden. <br>\n",
        "Da hier eine Sequenz an verschiedenen Transformationen durchgeführt werden soll, bietet sich die Anwendung von SciKit-Learn's Pipeline an. <br>\n",
        "\n",
        "\n",
        "#### NaNs\n",
        "Für den Umgang mit NaNs gibt es einige Möglichkeiten: <br>\n",
        "$\\rightarrow~$ Entfernen der jeweiligen Instanzen (Pandas `dropna()`). <br>\n",
        "$\\rightarrow~$ Entfernen des ganzen Features (Pandas `drop()`). <br>\n",
        "$\\rightarrow~$ Ersetzen durch einen bestimmten Wert (Null, den Median, den Mittelwert mittels Pandas `fillna()` oder Scikit-Learns SimpleImputer Transformer)<br>\n",
        "Im folgenden Beispiel werden wir NaNs durch den Median ersetzen. <br>\n",
        "\n",
        "#### Ausreißer\n",
        "Um Ausreißer zu detektieren kann die $\\sigma$-Regel verwendet werden. Dabei wird der sogenannte Z-Score für jeden Datenpunkt berechnet (die Entfernung eines Datenpunktes zum Mittelwert ausgedrückt in Standardabweichungen) und falls der Betrag des Z-Scores einen Grenzwert überschreitet (z.B. drei, heißt drei Standardabweichungen von Mittelwert entfernt) kann dieser Datenpunkt als Ausreißer betrachtet werden. <br>\n",
        "Eine andere Möglichkeit wäre einfach ein gültigen Wertebereich von Hand zu definieren: [$x_{\\text{min}},x_{\\text{max}}$] und alle Werte außerhalb des Gültigkeitsbereichs als Aureißer zu betrachten. <br>\n",
        "Sind Ausreißer erkannt worden, können diese genau wie NaNs behandelt werden. Ausreißer werden in diesem Beispiel durch den Median ersetzt. <br>\n",
        "\n",
        "\n",
        "#### Rauschen\n",
        "Anschließend soll kurz auf die Behandlung von Rauschen eingegangen werden. Rauschen kann bereits Hardwareseitig durch Frequenzfilter (z.B. diskrete Lineare Filter wie den Butterworth-Filter oder den exponentiellen Filter) entfernt werden. Aber auch Softwareseitig werden verschiedene Filtermethoden angeboten. In der Regel werden bei der Anwendung solcher Filter auf Zeitreihen alle Daten verändert. Ein Beispiel ist der gleitenden Mittelwert oder der gleitenden Median. Letzteres soll hier vorgestellt werden, da Pandas diesen bereits mitbringt. <br>\n",
        "\n",
        "An dieser Stelle möchte ich erwähnen das die Behandlung von Rauschen hier nur zu demonstrationszwecken gezeigt werden soll. In einem Realfall wäre der Einsatz für diesen Datensatz mehr als fraglich, da der gleitende Mittelwert hier über verschiede Batches, verschiedene Gase und verschiedene Konzentrationsstufen hinaus berechnet wird, was offensichtlich nicht sinnvoll ist. <br>\n",
        "\n",
        "\n",
        "#### Normalisierung und Skalierung\n",
        "Im letzten Schritt der Datenbereinigung werden wir die Daten skalieren. Verschiedene Features im Datensatz können Werte in verschiedenen Bereichen aufweisen. So kann beispielsweise in einem Mitarbeiterdatensatz der Gehaltsbereich von Tausend bis Hunderttausend liegen, der Wertebereich des Altersmerkmals aber nur zwischen 20-70. Das heißt, ein Feature ist im Vergleich zum anderen stärker gewichtet. Anwendungen statistischer Methoden können in solchen Situationen unerwünschte und unbrauchbare Ergebnisse liefern. Anwendungen bei denen eine Featureskalierung wichtig ist sind <br>\n",
        "$\\rightarrow~$ k-nearest neighbors oder k-means mit euklidischem Abstandsmaß; <br>\n",
        "$\\rightarrow~$ Logistische Regression, SVM, Perzeptrons oder neuronale Netze bei denen Gradient Descent verwendet wird; <br>\n",
        "$\\rightarrow~$ lineare Diskriminanzanalyse, Hauptkomponentenanalyse oder Kernel-Hauptkomponentenanalyse. <br>\n",
        "\n",
        "Um das zu vermeiden empfiehlt es sich die Daten zu transformieren. Gängige Transformationen sind <br>\n",
        "\n",
        "$\\rightarrow~$ Standardisierung: skalieren der Daten auf die Normalverteilung $\\mathcal{N}(0,1)$; <br>\n",
        "$\\rightarrow~$ Min/Max-Normalisierung: skalieren der Daten auf ein fixen Bereich, z.B. [0,1]; <br> \n",
        "$\\rightarrow~$ $\\mathcal{l}_{1}$-/$\\mathcal{l}_{2}$-Normalisierung: Normalisierung des Featurevektors auf den Einheitsvektor (bei der $\\mathcal{l}_{1}$-Normalisierung summiert sich der Absolutwert jedes Elements auf 1 und bei der $\\mathcal{l}_{2}$-Normalisierung summiert sich die Quadratsumme zu 1).\n",
        "\n",
        "Welche letztlich die beste Wahl ist hängt stets von der Situation ab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu5JTCHUzrxT"
      },
      "source": [
        "### Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VUbAsebzrxU"
      },
      "source": [
        "Das hier gezeigte Vorgehen versteift sich auf die Anwendung von Pipelines (siehe Backup der Modul Daten Präsentation). <br>\n",
        "Diese können eine Sequenz von Transformationen (sogenannte Transformatoren) abarbeiten. Zudem können schnell und einfach eigene Transformatoren geschrieben werden, wie hier gezeigt für den `RollingMean`, die `OutlierDetection` und den `GasLabel` Transformer. <br>\n",
        "\n",
        "Das implementieren eines eigenen Transformers soll am Beispiel `RollingMean` kurz beschrieben werden. Als erstes wird eine Klasse mit dem Namen RollingMean definiert, welche von den Basisklassen BaseEstimator und TransformerMixin erbt. Die `fit()` Methode gibt einfach self zurück und lediglich die `transform()` Methode wird der Anforderung nach angepasst. Hier wird ein DataFrame Objekt zurückgegeben welches den asymmetrischen gleitenden Mittelwert enthält."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lyqOBplzrxW"
      },
      "source": [
        "```python\n",
        "class RollingMean(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, window_size=3):\n",
        "        self._window = window_size\n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        df_temp = pd.DataFrame(X)\n",
        "        return df_temp.rolling(window=3, min_periods=1).mean().to_numpy()\n",
        "\n",
        "class OutlierDetection(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, std):\n",
        "        self._std = std\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        mask = np.abs((X - X.mean(0)) / X.std(0)) > 3\n",
        "        return np.where(mask, np.median(X, axis=0), X)\n",
        "    \n",
        " ```   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaIspdZJzrxW"
      },
      "source": [
        "Als erstes sollte berücksichtigt werden, dass numerische Features anders als (textbasierte) kategorische Features oder Klassenlabels tranformieren. Wir trennen daher die Klassenlabels von den restlichen Daten, Prädiktoren genannt. Die Prädiktoren können je nach Typ, numerisch oder textbasiert, weiter getrennt werden. In unserem Beispiel sind die Prädiktoren numerisch. Wir trennen also lediglich die Klassenlabels von den Prädikatoren und transformieren beide getrennt voneinander durch unterschiedliche Transformatoren und Pipelines. <br>\n",
        "\n",
        "Wir definieren uns eine Liste mit Prädiktoren die transformiert werden sollen. Alle anderen werden nicht weiter berücksichtigt. Wir bezeichnen diese Liste als num_features und tragen dabei die Namen der Features ein, hier die `DR` Messungen der ersten vier Sensoren. Da wir auch die Klassenlabels transofmieren möchten erstellen wir für diese ebenfallls eine Liste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5RwtSEHzrxY"
      },
      "source": [
        "num_features = ['DR_1', 'DR_2', 'DR_3', 'DR_4']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5gkGo-lzrxc"
      },
      "source": [
        "#### Transformationen der numerischen Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56XgO7V_zrxd"
      },
      "source": [
        "Anschließend definieren wir die Pipeline der numerischen Transformationen als Liste. Diese erhält den Namen des Transformers (beliebig) sowie den Transformer. Dies legt auch die Reihenfolge der durchzuführenden transformationen fest.\n",
        "```python\n",
        "num_pipeline = Pipeline([\n",
        "\t\t('mean_imputer', SimpleImputer(strategy='median')),\n",
        "\t\t('outlier_detection', OutlierDetection(std=3)),\n",
        "\t\t('rolling_mean', RollingMean(3)),\n",
        "\t\t('scaler', MinMaxScaler(feature_range=(0,1))),\n",
        "\t])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUEA9R8vzrxe"
      },
      "source": [
        "$\\rightarrow~$ SimpleImputer: ersetzt NaNs durch den Median. <br>\n",
        "$\\rightarrow~$ OutlierDetection: detektiert Ausreißer nach der 3-$\\sigma$ Regel und ersetzt diese durch den jeweiligen Median des Features. Die Implementierung ist oben gezeigt. <br>\n",
        "$\\rightarrow~$ RollingMean: behandelt Daten mit rauschen mit dem gleitenden Median und einer Fenstergröße von drei Werten. <br>\n",
        "$\\rightarrow~$ MinMaxScaler: Skaliert uns die Daten auf ein gewünschtes Intervall, hier auf das Intervall [0,1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbDN3q40zrxf"
      },
      "source": [
        "#### Transformationen der Klassenlabels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZnI4xu-zrxf"
      },
      "source": [
        "Erstellen wir noch eine Pipeline, welche die transformationen der Klassenlabels festlegt.\n",
        "```python\n",
        "target_pipeline = Pipeline([\n",
        "\t\t('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "\t\t('one_hot_encoder', OneHotEncoder(sparse=False, categories='auto')),\n",
        "\t\t('sorter', GasLabel()),\n",
        "\t])\n",
        "    \n",
        "```    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NzuxlLXzrxg"
      },
      "source": [
        "$\\rightarrow~$ SimpleImputer: ersetzen fehlender Werte durch den meist auftretenden Wert (äquivalent zur Mode, andere statistische Maße die genutzt werden können sind Mean oder Median). Der SimpleImputer soll hier nur zu Präsentationszwecken gezeigt werden, da keine Klassenlabels im vorliegenden Beispieldatensatz fehlen. <br>\n",
        "$\\rightarrow~$ OneHotEncoder: überführt textbasierte kategorische Daten zunächst in numerische Kategorien und One-Hot encoded diese anschließend. Das heißt, es werden $n$ binäre Spaltenvektoren ($n$ Anzahl der verschiedenen Klassenlabels, hier 18) erzeugt. Das One-Hot encoding macht Sinn, da die meisten Algorithmen des Machine-Learnings den Umgang mit Zahlen bevorzugen. <br>\n",
        "$\\rightarrow~$ Zuletzt habe ich den `GasLabel` Transformer implementiert. Dieser fügt am Ende eine weitere Spalte hinzu und ist vom Prinzip her das selbe wie Scikit-Learns `LabelEncoder` (dieser funktioniert im aktuellen Release nicht mit dem ColumnTransformer den wir später verwenden werden). Dabei wird jeder Kategorie eine Zahl zugeordned, aufsteigend und beginnend mit eins. Dies wird uns später bei der Datenvisualisierung helfen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSdGqLAvzrxi"
      },
      "source": [
        "#### ColumnTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQGrbD5Dzrxj"
      },
      "source": [
        "Sind beide Pipelines definiert, führen wir diese mit dem `ColumnTransfromer` zusammen und geben für jede Pipelinde noch die zu transformierende Liste mit. <br>\n",
        "```python\n",
        "full_pipeline = ColumnTransformer(\n",
        "\t\ttransformers=[\n",
        "\t\t\t('num', num_pipeline, num_features),\n",
        "\t\t\t('target', target_pipeline, target)],\n",
        "\t\tremainder='drop')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqLv0OzOzrxk"
      },
      "source": [
        "Transformieren wir die Trainungs- und Testdaten. Beachte, wir erhalten kein Pandas DataFrame Objekt als Rückgabewert sondern NumPy Arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkc1kmYDzrxl"
      },
      "source": [
        "import scripts.clean\n",
        "import numpy as np\n",
        "\n",
        "np_train_prepared,np_test_prepared = scripts.clean.clean(df_training,df_test,num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR7MpyVozrxp"
      },
      "source": [
        "Überprüfen wir ob die Dimensionen stimmen und ob noch NaNs vorhanden sind."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI3BUr_Kzrxq"
      },
      "source": [
        "np_train_prepared.shape, np_test_prepared.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGjJigEIzrxu"
      },
      "source": [
        "np.isnan(np.min(np_train_prepared)), np.isnan(np.min(np_test_prepared))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJGqb_epzrxz"
      },
      "source": [
        "Untersuchen wir die ersten zwei Zeilen der transformierten Trainingsdaten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdVsHHWwzrx1"
      },
      "source": [
        "np_train_prepared[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQOIBnBMzrx6"
      },
      "source": [
        "Sollte eine fortgeschrittene Feature Selection Teil der Pipeline sein, kann auf https://scikit-learn.org/stable/modules/feature_selection.html mehr dazu gelesen werden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYE-R8zXzrx7"
      },
      "source": [
        "df_training.shape, df_original.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9EH_TVCzrx-"
      },
      "source": [
        "## Speichern der bereinigten Daten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsqfDonbzrx_"
      },
      "source": [
        "Speichern wir Daten im csv und pickle Format ab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_mlHIMyzryA"
      },
      "source": [
        "scripts.clean.safe(np_train_prepared,np_test_prepared)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAZLLxkRzryE"
      },
      "source": [
        "Nach dem bereinigen, transformieren und speichern der Daten, widmen wir uns als nächstes verschiedene Möglichkeiten der Datenvisualisierung."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_AxyWq1zryG"
      },
      "source": [
        "## Do it yourself:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q4VY_hCzryH"
      },
      "source": [
        "Ändern Sie die Features und überprüfen Sie ob die Pipeline immer noch funktioniert. Was sind die Vorteile einer einer Pipeline?\n",
        "Nutzen Sie statt den Sensoren 1-4 die Sensoren 8-12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_l4pnUdzryH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}